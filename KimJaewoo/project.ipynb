{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "id": "initial_id"
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib\n",
    "import matplotlib.font_manager as fm\n",
    "from sklearn.metrics import f1_score\n",
    "## 맥북용 한글\n",
    "# mpl.rcParams['font.family'] = 'AppleGothic'\n",
    "# mpl.rcParams['axes.unicode_minus'] = False   # 마이너스(-) 부호 깨짐 방지"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import os\n",
    "\n",
    "# 사용할 한글 폰트 경로 (나눔고딕 또는 맑은고딕)\n",
    "font_path = 'C:\\\\Windows\\\\Fonts\\\\malgun.ttf'  #\n",
    "\n",
    "# 폰트 파일이 있는지 확인 후 적용\n",
    "if os.path.exists(font_path):\n",
    "    fontprop = fm.FontProperties(fname=font_path)\n",
    "    plt.rcParams['font.family'] = fontprop.get_name()\n",
    "    plt.rcParams['axes.unicode_minus'] = False  # 마이너스 깨짐 방지\n",
    "else:\n",
    "    print(\"폰트 파일이 존재하지 않습니다. 경로를 확인하세요.\")\n",
    "\n",
    "# 테스트\n",
    "plt.plot([1, 2, 3], [1, 2, 3])\n",
    "plt.title(\"한글 테스트\")\n",
    "plt.show()"
   ],
   "id": "413d8106f5943b3f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "634bc618f79b57fe"
   },
   "cell_type": "markdown",
   "source": [],
   "id": "634bc618f79b57fe"
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "ae20c59da18cab29",
    "outputId": "f0aa0310-f382-4f9f-a51b-8ec5402800f9"
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_csv('data/dataset.csv')\n",
    "df.head()"
   ],
   "id": "ae20c59da18cab29",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cab573b9640ec20",
    "outputId": "ee79c0df-2814-41af-e27b-5aa703d69e66"
   },
   "cell_type": "code",
   "source": [
    "df.info()\n",
    "# 데이터 개수는 4424"
   ],
   "id": "cab573b9640ec20",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "f5d237fe0f17036",
    "outputId": "ff281a0f-e72c-47b1-894d-7f3e752592ee"
   },
   "cell_type": "code",
   "source": [
    "df.describe()"
   ],
   "id": "f5d237fe0f17036",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "56797ba3176c251a",
    "outputId": "470798a5-3bdb-4c73-f0b9-86c49aa0d5f0"
   },
   "cell_type": "code",
   "source": [
    "df.isna().sum()\n",
    "# 컬럼에 결측치 확인"
   ],
   "id": "56797ba3176c251a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "77299858b181f6c3",
    "outputId": "57b19539-a7f1-4ac6-e2d9-bbc9f1a6eb88"
   },
   "cell_type": "code",
   "source": [
    "# 불필요한 컬럼(열) 삭제\n",
    "df = df.drop(['Application mode'], axis=1)\n",
    "df = df.drop(['Application order'], axis=1)\n",
    "df = df.drop(['Nacionality'], axis=1)\n",
    "df = df.drop(['Mother\\'s qualification'], axis=1)\n",
    "df = df.drop(['Father\\'s qualification'], axis=1)\n",
    "df = df.drop(['International'], axis=1)\n",
    "df = df.drop(['Curricular units 1st sem (credited)'], axis=1)\n",
    "df = df.drop(['Curricular units 1st sem (enrolled)'], axis=1)\n",
    "df = df.drop(['Curricular units 1st sem (evaluations)'], axis=1)\n",
    "df = df.drop(['Curricular units 1st sem (without evaluations)'], axis=1)\n",
    "df = df.drop(['Curricular units 2nd sem (credited)'], axis=1)\n",
    "df = df.drop(['Curricular units 2nd sem (enrolled)'], axis=1)\n",
    "df = df.drop(['Curricular units 2nd sem (evaluations)'], axis=1)\n",
    "df = df.drop(['Curricular units 2nd sem (without evaluations)'], axis=1)\n",
    "df = df.drop(['Unemployment rate'], axis=1)\n",
    "df = df.drop(['Inflation rate'], axis=1)\n",
    "df = df.drop(['GDP'], axis=1)\n",
    "\n",
    "df.head()\n"
   ],
   "id": "77299858b181f6c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "52612e2f634ce1fd"
   },
   "cell_type": "code",
   "source": [
    "# target에 범주를 매핑(데이터 설명에 나와있는 범주 A,B,C -> 0,1,2)\n",
    "targetvariable = {'Dropout': 0,'Graduate': 1,'Enrolled': 2}\n",
    "df['Target'] = df['Target'].map(targetvariable)"
   ],
   "id": "52612e2f634ce1fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df.rename(columns={\n",
    "    'Marital status': '결혼 상태',\n",
    "    'Application mode': '지원 방법',\n",
    "    'Application order': '지원 순서',\n",
    "    'Course': '수강 과정',\n",
    "    'Daytime/evening attendance': '주/야간 수업',\n",
    "    'Previous qualification': '이전 학력',\n",
    "    'Nationality': '국적',\n",
    "    \"Mother's qualification\": '어머니 학력',\n",
    "    \"Father's qualification\": '아버지 학력',\n",
    "    \"Mother's occupation\": '어머니 직업',\n",
    "    \"Father's occupation\": '아버지 직업',\n",
    "    'Displaced': '이재민 여부',\n",
    "    'Educational special needs': '특수 교육 필요 여부',\n",
    "    'Debtor': '연체 여부',\n",
    "    'Tuition fees up to date': '등록금 납부 여부',\n",
    "    'Gender': '성별',\n",
    "    'Scholarship holder': '장학금 수혜 여부',\n",
    "    'Age': '입학 나이',\n",
    "    'International': '국제 학생 여부',\n",
    "    'Curricular units 1st sem (credited)': '1학기 학점 인정 과목 수',\n",
    "    'Curricular units 1st sem (enrolled)': '1학기 수강 과목 수',\n",
    "    'Curricular units 1st sem (evaluations)': '1학기 평가된 과목 수',\n",
    "    'Curricular units 1st sem (approved)': '1학기 합격 과목 수',\n",
    "    'Curricular units 1st sem (grade)': '1학기 성적 평균',\n",
    "    'Curricular units 1st sem (without evaluations)': '1학기 평가 미진행 과목 수',\n",
    "    'Curricular units 2nd sem (credited)': '2학기 학점 인정 과목 수',\n",
    "    'Curricular units 2nd sem (enrolled)': '2학기 수강 과목 수',\n",
    "    'Curricular units 2nd sem (evaluations)': '2학기 평가된 과목 수',\n",
    "    'Curricular units 2nd sem (approved)': '2학기 합격 과목 수',\n",
    "    'Curricular units 2nd sem (grade)': '2학기 성적 평균',\n",
    "    'Curricular units 2nd sem (without evaluations)': '2학기 평가 미진행 과목 수',\n",
    "    'Unemployment rate': '실업률',\n",
    "    'Inflation rate': '인플레이션율',\n",
    "    'GDP': 'GDP',\n",
    "    'Target': '학업 성취도 (타겟)'\n",
    "}, inplace=True)\n"
   ],
   "id": "81d14bc946ab49f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 911
    },
    "id": "9ab1849e5d40d9e3",
    "outputId": "5a30bf7f-a72a-43e1-9541-a1c42e9c00bb"
   },
   "cell_type": "code",
   "source": [
    "corr_mat = df.corr(numeric_only=True)\n",
    "corr_mat"
   ],
   "id": "9ab1849e5d40d9e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 682
    },
    "id": "5caa20bcf2d60ff1",
    "outputId": "d9bfc083-a0ae-4ef9-d739-fb2b3f51e5f9"
   },
   "cell_type": "code",
   "source": [
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(corr_mat, annot=True, cmap='coolwarm', fmt='.2f', annot_kws={'size': 8})\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.xticks(rotation=24)\n",
    "# plt.savefig('heatmap_pre.png', dpi=200)\n",
    "plt.show()"
   ],
   "id": "5caa20bcf2d60ff1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "id": "6b670369ec829407"
   },
   "cell_type": "markdown",
   "source": [
    "# 데이터 분할"
   ],
   "id": "6b670369ec829407"
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install catboost\n",
    "!pip install xgboost\n",
    "!pip install lightgbm"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7klxeRS_xOr",
    "outputId": "9d2eeea7-f9f5-4825-8141-fef5aa5039b6"
   },
   "id": "-7klxeRS_xOr",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "df = df[df['Target'] != 2]\n",
    "\n",
    "df.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 278
    },
    "id": "oJ5sZ-hIBBti",
    "outputId": "05ccfa48-fdc1-48e1-c59d-fcd4f321a1de"
   },
   "id": "oJ5sZ-hIBBti",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42, stratify=y_train)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_val.shape, y_val.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PE8h3CJ5Dqef",
    "outputId": "b41dcfe4-a2ef-43b4-ae61-af2bb92e6236"
   },
   "id": "PE8h3CJ5Dqef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC"
   ],
   "id": "938325f036823da3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "\n",
    "# 범주형 변수 목록 (사용자 정의)\n",
    "cat_cols = [\n",
    "    'Marital status',\n",
    "    'Course',\n",
    "    'Daytime/evening attendance',\n",
    "    'Previous qualification',\n",
    "    \"Mother's occupation\",\n",
    "    \"Father's occupation\",\n",
    "    'Displaced',\n",
    "    'Educational special needs',\n",
    "    'Debtor',\n",
    "    'Tuition fees up to date',\n",
    "    'Gender',\n",
    "    'Scholarship holder'\n",
    "]\n",
    "\n",
    "# 수치형 변수 목록 (cat_cols 제외한 나머지)\n",
    "\n",
    "num_cols = [\n",
    "    'Age',\n",
    "    'Curricular units 1st sem (approved)',\n",
    "    'Curricular units 1st sem (grade)',\n",
    "    'Curricular units 2nd sem (approved)',\n",
    "    'Curricular units 2nd sem (grade)'\n",
    "]\n",
    "\n",
    "\n",
    "# 전처리 파이프라인 정의\n",
    "numeric_transformer = Pipeline([('scaler', StandardScaler())])\n",
    "\n",
    "categorical_transformer = Pipeline([('onehot', OneHotEncoder(handle_unknown='ignore',sparse_output=False))])\n",
    "\n",
    "preprocessor = ColumnTransformer([('num', numeric_transformer, num_cols),('cat', categorical_transformer, cat_cols)]).set_output(transform='pandas')\n",
    "\n",
    "# 파이프라인 정의 (scaler 대신 preprocessor)\n",
    "pipeline = Pipeline([('preprocessor', preprocessor),('classifier', LogisticRegression())])\n",
    "\n",
    "\n",
    "param_grid = [\n",
    "  # LogisticRegression\n",
    "  {\n",
    "        'classifier': [LogisticRegression(max_iter=1000, random_state=42)],\n",
    "        'classifier__C': [0.01, 0.1, 1, 10],\n",
    "        'classifier__solver': ['liblinear', 'lbfgs'],\n",
    "    },\n",
    "  # SVC\n",
    "  {\n",
    "        'classifier': [SVC(random_state=42)],\n",
    "        'classifier__kernel': ['linear', 'rbf'],\n",
    "        'classifier__C': [0.1, 1, 10],\n",
    "    },\n",
    "  # KNeighborsClassifier\n",
    "  {\n",
    "        'classifier': [KNeighborsClassifier()],\n",
    "        'classifier__n_neighbors': [3, 5, 7],\n",
    "        'classifier__weights': ['uniform', 'distance'],\n",
    "    },\n",
    "\n",
    "  # RandomForestClassifier (전처리 그대로 둬도 무방, 스케일링 영향 적음)\n",
    "  {\n",
    "        'classifier': [RandomForestClassifier(random_state=42)],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [None, 10, 20],\n",
    "    },\n",
    "  # XGBClassifier\n",
    "  {\n",
    "        'classifier': [XGBClassifier(eval_metric='logloss', random_state=42, n_jobs=-1)],\n",
    "        'classifier__n_estimators': [100, 200, 300, 500],\n",
    "        'classifier__max_depth': [3, 5, 7, 9],\n",
    "        'classifier__learning_rate': [0.05, 0.1, 0.2],\n",
    "     },\n",
    "  # LGBMClassifier\n",
    "  {\n",
    "        'classifier': [LGBMClassifier(random_state=42, n_jobs=-1, verbose=-1, feature_name='auto')],\n",
    "        'classifier__n_estimators': [100, 200, 300, 400],\n",
    "        'classifier__max_depth': [-1, 5, 10, 10],\n",
    "        'classifier__num_leaves': [20, 31, 40, 50],\n",
    "        'classifier__learning_rate': [0.01, 0.05, 0.1],\n",
    "        'classifier__subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'classifier__colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "        'classifier__reg_alpha': [0, 0.1, 0.01],\n",
    "        'classifier__reg_lambda': [0, 0.1, 0.01]\n",
    "    },\n",
    "  # CatBoostClassifier\n",
    "  {\n",
    "        'classifier': [CatBoostClassifier(verbose=0, random_state=42)],\n",
    "        'classifier__iterations': [50, 100, 200, 500],\n",
    "        'classifier__depth': [4, 6, 8 ,10],\n",
    "        'classifier__learning_rate': [0.01, 0.05,  0.1],\n",
    "    }\n",
    "\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "grid_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_grid,\n",
    "    cv=cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=2,\n",
    "    scoring='f1'\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"최적 검증점수: {grid_search.best_score_:.4f}\")\n",
    "print(f\"최적 파라미터: {grid_search.best_params_}\")\n",
    "\n",
    "val_score = grid_search.score(X_val, y_val)\n",
    "print(f\"Validation Accuracy with best params: {val_score:.4f}\")\n"
   ],
   "id": "16da6a63535aed2f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. grid_search 결과 DataFrame 변환 및 모델별 최고 점수/파라미터 출력\n",
    "results_df = pd.DataFrame(grid_search.cv_results_)\n",
    "results_df['classifier_name'] = results_df['param_classifier'].apply(lambda x: type(x).__name__)\n",
    "\n",
    "print(\"===== 모델별 최고 CV 점수 및 하이퍼파라미터 =====\\n\")\n",
    "for clf_name, group in results_df.groupby('classifier_name'):\n",
    "    idx = group['mean_test_score'].idxmax()\n",
    "    best_row = group.loc[idx]\n",
    "    print(f\"모델: {clf_name}\")\n",
    "    print(f\"  최고 검증점수: {best_row['mean_test_score']:.4f}\")\n",
    "    print(f\"  최고 파라미터: {best_row['params']}\\n\")\n",
    "\n",
    "# 2. 최적 모델로 X_val 예측 (항상 파이프라인 전체에 원본 X_val을 넣어야 함)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# predict_proba 지원 여부 확인\n",
    "if hasattr(best_model.named_steps['classifier'], 'predict_proba'):\n",
    "    probs = best_model.predict_proba(X_val)[:, 1]\n",
    "    preds = (probs >= 0.5).astype(int)\n",
    "else:\n",
    "    # SVC 등 일부 모델은 predict_proba 미지원, predict 사용\n",
    "    preds = best_model.predict(X_val)\n",
    "    # 만약 SVC(kernel='linear') 등 이진분류에서 decision_function이 있으면\n",
    "    # threshold 적용이 가능하지만, 여기서는 predict 결과 사용\n",
    "\n",
    "# 3. 평가 지표 출력\n",
    "accuracy = accuracy_score(y_val, preds)\n",
    "f1 = f1_score(y_val, preds)\n",
    "\n",
    "print(f\"\\n[Best Model 평가 결과]\")\n",
    "print(f\"Validation Accuracy with threshold 0.5: {accuracy:.4f}\")\n",
    "print(f\"Validation F1 Score with threshold 0.5: {f1:.4f}\")"
   ],
   "id": "5e639d5b6e9c239d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import joblib\n",
    "\n",
    "# grid_search.best_estimator_는 최적의 파이프라인 전체(전처리+모델)를 포함합니다.\n",
    "joblib.dump(grid_search.best_estimator_, 'best_model_pipeline.pkl')"
   ],
   "id": "f57efb96e4b6f180",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 저장한 모델 불러오기\n",
    "loaded_model = joblib.load('best_model_pipeline.pkl')\n",
    "\n",
    "# 바로 예측 가능 (전처리 자동 적용)\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "y_pred"
   ],
   "id": "ba86d389a024519a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "test_score = best_model.score(X_val, y_val)\n",
    "print(f\"Test Accuracy: {test_score:.4f}\")"
   ],
   "id": "83a33ee95ff2b6fd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 딥러닝",
   "id": "d3a2c21938b5d982"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "deep_df = df.copy()",
   "id": "82d5db0cdfe67fb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "deep_df = deep_df [deep_df ['Target'] != 2].copy()",
   "id": "3063f2c94411490e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "deep_df['Target'].value_counts()",
   "id": "81b87ee702f36499",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 범주형 변수 목록 (사용자 정의)\n",
    "cat_cols = [\n",
    "    'Marital status',\n",
    "    'Course',\n",
    "    'Daytime/evening attendance',\n",
    "    'Previous qualification',\n",
    "    \"Mother's occupation\",\n",
    "    \"Father's occupation\",\n",
    "    'Displaced',\n",
    "    'Educational special needs',\n",
    "    'Debtor',\n",
    "    'Tuition fees up to date',\n",
    "    'Gender',\n",
    "    'Scholarship holder'\n",
    "]\n",
    "\n",
    "# 수치형 변수 리스트\n",
    "num_cols = [col for col in deep_df.columns if col not in cat_cols + ['Target']]"
   ],
   "id": "43bea167aacbce84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "scaler = StandardScaler()\n",
    "deep_df[num_cols] = scaler.fit_transform(deep_df[num_cols])"
   ],
   "id": "806c33c7720d13e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = deep_df.drop(columns=['Target']).values\n",
    "y = deep_df['Target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42, stratify=y_train)"
   ],
   "id": "534dc61c59c0d377",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class StudentDropoutDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)  # CrossEntropyLoss를 위해 long 사용\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = StudentDropoutDataset(X_tr, y_tr)       # 실제 학습용(train)\n",
    "val_dataset =  StudentDropoutDataset(X_val, y_val)      # 검증용(validation)\n",
    "test_dataset =  StudentDropoutDataset(X_test, y_test)   # 테스트용(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
   ],
   "id": "8004ca4e6cf9a58f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 2)  # 이진분류(0,1)니까 2 출력\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "2ad461c6eab102b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DropoutNet(nn.Module):\n",
    "    def __init__(self, input_dim, dropout_rate=0.2):\n",
    "        super(DropoutNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ],
   "id": "51a137054e95eaa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_by_scheduler(model, train_loader, val_loader, criterion, optimizer, scheduler, device, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        # === 학습 ===\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = X_batch.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        # === 검증 ===\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                batch_size = X_batch.size(0)\n",
    "                val_running_loss += loss.item() * batch_size\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += batch_size\n",
    "\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        # 학습률 스케줄러 업데이트\n",
    "        scheduler.step()\n",
    "\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f} | LR: {lr:.6f}\")"
   ],
   "id": "6b5771682118b8a8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_with_early_stopping(\n",
    "    model, train_loader, val_loader, criterion, optimizer, scheduler, device,\n",
    "    epochs=100, patience=10, min_delta=1e-4\n",
    "):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # === 학습 ===\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batch_size = X_batch.size(0)\n",
    "            running_loss += loss.item() * batch_size\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == y_batch).sum().item()\n",
    "            total += batch_size\n",
    "\n",
    "        train_loss = running_loss / total\n",
    "        train_accuracy = correct / total\n",
    "\n",
    "        # === 검증 ===\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "\n",
    "                batch_size = X_batch.size(0)\n",
    "                val_running_loss += loss.item() * batch_size\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "                val_total += batch_size\n",
    "\n",
    "        val_loss = val_running_loss / val_total\n",
    "        val_accuracy = val_correct / val_total\n",
    "\n",
    "        scheduler.step()\n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{epochs} - \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f} | LR: {lr:.6f}\")\n",
    "\n",
    "        # === Early Stopping 체크 ===\n",
    "        if best_val_loss - val_loss > min_delta:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # 필요하다면 모델 가중치 저장 가능\n",
    "            # torch.save(model.state_dict(), 'best_model.pt')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
    "                break"
   ],
   "id": "2b5a585a53d91b64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DropoutNet(input_dim=X_tr.shape[1], dropout_rate=0.3).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "\n",
    "train_with_early_stopping(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    epochs=100,         # 최대 에폭\n",
    "    patience=10,        # 개선 없으면 10번 후 종료\n",
    "    min_delta=1e-4      # 개선 기준\n",
    ")"
   ],
   "id": "59c7e65efd9d921b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
